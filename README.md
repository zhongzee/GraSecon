<!-- PROJECT LOGO -->

<p align="center">
  <h1 align="center"><img src="materials/teaser.png" width="256"></h1>
  <h1 align="center">UnSec: Semantic Hierarchy Nexus for Open-vocabulary Object Detection</h1>
  <p align="center">
    <a href="https://oatmealliu.github.io/"><strong>Mingxuan Liu</strong></a>
    Â·
    <a href="https://tyler-hayes.github.io/"><strong>Tyler L. Hayes</strong></a>
    Â·
    <a href="https://scholar.google.ca/citations?user=xf1T870AAAAJ&hl=en"><strong>Elisa Ricci</strong></a>
    Â·
    <a href="https://scholar.google.fr/citations?user=PXm1lPAAAAAJ&hl=fr"><strong>Gabriela Csurka</strong></a>
    Â·
    <a href="https://ricvolpi.github.io/"><strong>Riccardo Volpi</strong></a>
  </p>
  <h2 align="center">CVPR 2024 âœ¨Highlightâœ¨</h2>
  <h3 align="center">
    <a href="https://arxiv.org/abs/2405.10053">Paper</a> |
    <a href="https://arxiv.org/abs/2405.10053">ArXiv</a> |
    <a href="https://github.com/naver/UnSec">Code</a> |
    <a href="">Poster (coming soon)</a>
  </h3> 
<div align="center"></div>
<p align="center">
<p>

<h1 align="center"><img src="materials/approach.png"></h1>


[//]: # (- [01/15/2024] Our work is accepted to <a href="https://iclr.cc/Conferences/2024"><strong>ICLR 2024</strong></a> ðŸŒ¼! Code is coming soon. See you in Vienna this May!)

## Installation
Requirements:
- Linux or macOS with Python â‰¥ 3.8
- PyTorch â‰¥ 1.8.2.
  Install them together at [pytorch.org](https://pytorch.org) to make sure of this. Note, please check
  PyTorch version matches that is required by Detectron2.
- Detectron2: follow [Detectron2 installation instructions](https://detectron2.readthedocs.io/tutorials/install.html).
- OpenAI API (optional, if you want to construct hierarchies using LMMs)

Setup environment
```shell script
# Clone this project repository under your workspace folder
git clone https://github.com/naver/UnSec.git --recurse-submodules
cd UnSec
# Create conda environment and install the dependencies
conda env create -n UnSec -f UnSec.yml
# Activate the working environment
conda activate UnSec
# Install Detectron2 under your workspace folder
# (Please follow Detectron2 official instructions)
cd ..
git clone git@github.com:facebookresearch/detectron2.git
cd detectron2
pip install -e .
```
Our project uses two submodules,
[CenterNet2](https://github.com/xingyizhou/CenterNet2.git)
and
[Deformable-DETR](https://github.com/fundamentalvision/Deformable-DETR.git).
If you forget to add `--recurse-submodules`, do `git submodule init` and then `git submodule update`.

Set your OpenAI API Key to the environment variable (optional: if you want to generate hierarchies)
```shell script
export OPENAI_API_KEY=YOUR_OpenAI_Key
```

## OvOD Models Preparation
UnSec is training-free. So we just need to download off-the-shelf OvOD models and apply UnSec on top of them. 
You can download the models:
- [Detic](https://github.com/facebookresearch/Detic)
- [VLDet](https://github.com/clin1223/VLDet)
- [CoDet](https://github.com/CVMI-Lab/CoDet/tree/main?tab=readme-ov-file)

and put (or, softlink via `ln -s` command) under the `models` folder in this repository as:
```shell script
UnSec
    â””â”€â”€ models
          â”œâ”€â”€ codet
            â”œâ”€â”€ CoDet_OVLVIS_R5021k_4x_ft4x.pth
            â””â”€â”€ CoDet_OVLVIS_SwinB_4x_ft4x.pth
          â”œâ”€â”€ detic
            â”œâ”€â”€ coco_ovod
              â”œâ”€â”€ BoxSup_OVCOCO_CLIP_R50_1x.pth
              â”œâ”€â”€ Detic_OVCOCO_CLIP_R50_1x_caption.pth
              â”œâ”€â”€ Detic_OVCOCO_CLIP_R50_1x_max-size.pth
              â””â”€â”€ Detic_OVCOCO_CLIP_R50_1x_max-size_caption.pth
            â”œâ”€â”€ cross_eval
              â”œâ”€â”€ BoxSup-C2_L_CLIP_SwinB_896b32_4x.pth
              â”œâ”€â”€ BoxSup-C2_LCOCO_CLIP_SwinB_896b32_4x.pth
              â”œâ”€â”€ Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth
              â”œâ”€â”€ Detic_LI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth
              â””â”€â”€ Detic_LI_CLIP_SwinB_896b32_4x_ft4x_max-size.pth
            â”œâ”€â”€ lvis_ovod
              â”œâ”€â”€ BoxSup-C2_Lbase_CLIP_R5021k_640b64_4x.pth
              â”œâ”€â”€ BoxSup-C2_Lbase_CLIP_SwinB_896b32_4x.pth
              â”œâ”€â”€ Detic_LbaseCCcapimg_CLIP_R5021k_640b64_4x_ft4x_max-size.pth
              â”œâ”€â”€ Detic_LbaseCCimg_CLIP_R5021k_640b64_4x_ft4x_max-size.pth
              â”œâ”€â”€ Detic_LbaseI_CLIP_R5021k_640b64_4x_ft4x_max-size.pth
              â””â”€â”€ Detic_LbaseI_CLIP_SwinB_896b32_4x_ft4x_max-size.pth
            â”œâ”€â”€ lvis_std
              â”œâ”€â”€ BoxSup-C2_L_CLIP_R5021k_640b64_4x.pth
              â”œâ”€â”€ BoxSup-DeformDETR_L_R50_4x.pth
              â”œâ”€â”€ Detic_DeformDETR_LI_R50_4x_ft4x.pth
              â””â”€â”€ Detic_LI_CLIP_R5021k_640b64_4x_ft4x_max-size.pth
          â”œâ”€â”€ vldet
            â”œâ”€â”€ lvis_base.pth
            â”œâ”€â”€ lvis_base_swinB.pth
            â”œâ”€â”€ lvis_vldet.pth
            â””â”€â”€ lvis_vldet_swinB.pth
```


## Datasets Preparation
You can download the datasets:
- [iNat](https://drive.google.com/file/d/1Wav83umtIcITx9LqiWxh2LHXkZh3N40H/view?usp=drive_link): 17.8 GB
- [FSOD](https://drive.google.com/file/d/1XkdivbNsMSdOKk-zOMKjbudPo5VEvRjC/view?usp=drive_link): 14.7 GB
- [ImageNet-1k Val](https://drive.google.com/file/d/11cqx0wXjijnHyOXx7Coo2w1KfPVRuyL2/view?usp=drive_link): 6.2 GB

and put (or, softlink via `ln -s` command) under the `datasets` folder in this repository as:
```shell script
UnSec
    â””â”€â”€ datasets
          â”œâ”€â”€ inat
          â”œâ”€â”€ fsod
          â”œâ”€â”€ imagenet2012
```


## Run UnSec on OvOD
Example of applying UnSec on Detic for OvOD task using iNat dataset:
```shell script
# Vanilla OvOD (baseline)
bash scripts_local/Detic/inat/swin/baseline/inat_detic_SwinB_LVIS-IN-21K-COCO_baseline.sh
 
# UnSec using dataset-provided hierarchy
bash scripts_local/Detic/inat/swin/UnSec_gt/inat_detic_SwinB_LVIS-IN-21K-COCO_UnSec_gt.sh

# UnSec using LLM-generated synthetic hierarchy
bash scripts_local/Detic/inat/swin/UnSec_llm/inat_detic_SwinB_LVIS-IN-21K-COCO_UnSec_llm.sh
```

## Run UnSec on Zero-shot classification
Example of applying UnSec on CLIP zero-shot transfer task using ImageNet-1k dataset:
```shell script
# Vanilla CLIP Zero-shot transfer (baseline)
bash scripts_local/Classification/imagenet1k/baseline/imagenet1k_vitL14_baseline.sh

# UnSec using WordNet hierarchy
bash scripts_local/Classification/imagenet1k/UnSec_wordnet/imagenet1k_vitL14_UnSec_wordnet.sh

# UnSec using LLM-generated synthetic hierarchy
bash scripts_local/Classification/imagenet1k/UnSec_llm/imagenet1k_vitL14_UnSec_llm.sh
```


## UnSec Construction (optional)
Example of constructing UnSec classifier for OvOD task using iNat dataset:
```shell script
# UnSec using dataset-provided hierarchy
bash scripts_build_./nexus/inat/build_inat_nexus_gt.sh
# UnSec using LLM-generated synthetic hierarchy
bash scripts_build_./nexus/inat/build_inat_nexus_llm.sh
```

## Hierarchy Tree Planting (optional)
Example of building hierarchy trees using either dataset-provided or llm-generated hierarchy entities.

### Dataset-provided Hierarchy
```shell script
# Build hierarchy tree for iNat using dataset-provided hierarchy
bash scripts_plant_hrchy/inat/plant_inat_tree_gt.sh

# Build hierarchy tree for ImageNet-1k using WordNet hierarchy
bash scripts_plant_hrchy/imagenet1k/plant_imagenet1k_tree_wordnet.sh
```

### LLM-generated Hierarchy
```shell script
# Build hierarchy tree for iNat using LLM-generated synthetic hierarchy
bash scripts_plant_hrchy/inat/plant_inat_tree_llm.sh

# Build hierarchy tree for ImageNet-1k using LLM-generated synthetic hierarchy
bash scripts_plant_hrchy/imagenet1k/plant_imagenet1k_tree_llm.sh
```

## License
This project is licensed under the [LICENSE](LICENSE.txt) file.

## Citation
If you find our work useful for your research, please cite our paper using the following BibTeX entry:
```bibtex
@inproceedings{liu2024UnSec,
  title={{SH}i{N}e: Semantic Hierarchy Nexus for Open-vocabulary Object Detection},
  author={Liu, Mingxuan and Hayes, Tyler L. and Ricci, Elisa and Csurka, Gabriela and Volpi, Riccardo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2024},
}
```

## Acknowledgment
UnSec is built upon the awesome works
[iNat](https://github.com/visipedia/inat_loc),
[FSOD](https://github.com/fanq15/Few-Shot-Object-Detection-Dataset),
[BREEDS](https://github.com/MadryLab/BREEDS-Benchmarks),
[Hierarchy-CLIP](https://github.com/gyhandy/Hierarchy-CLIP),
[Detic](https://github.com/facebookresearch/Detic),
[VLDet](https://github.com/clin1223/VLDet),
and [CoDet](https://github.com/CVMI-Lab/CoDet).
We sincerely thank them for their work and contributions.

